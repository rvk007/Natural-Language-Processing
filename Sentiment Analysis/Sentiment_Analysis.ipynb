{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" Sentiment_Analysis.ipynb","provenance":[{"file_id":"1v8gTPLOKxT0Nw3gbnha33iDwTLKAFCfU","timestamp":1619761685339}],"collapsed_sections":[],"authorship_tag":"ABX9TyPcMuOwEO9HBr3FzZgc9tmB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"585Cd39_vqCm"},"source":["# Sentiment Analysis\n","Embedding-LSTM-Fully Connected  \n","\n","Dataset Preview\n","Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines.\n","\n","We will be using previous session tweet dataset. Let's just preview the dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"chtPDh1XRnCQ","executionInfo":{"status":"ok","timestamp":1619953308208,"user_tz":-330,"elapsed":5620,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"bf86d95c-a4e4-4ac3-9661-6af5c59e2e62"},"source":["pip install pytreebank"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytreebank\n","  Downloading https://files.pythonhosted.org/packages/e0/12/626ead6f6c0a0a9617396796b965961e9dfa5e78b36c17a81ea4c43554b1/pytreebank-0.2.7.tar.gz\n","Building wheels for collected packages: pytreebank\n","  Building wheel for pytreebank (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytreebank: filename=pytreebank-0.2.7-cp37-none-any.whl size=37070 sha256=0ddc68a9dec81b8a3549ccd412b98e22b8d76198a593cd7e2be7ff0c4d71b413\n","  Stored in directory: /root/.cache/pip/wheels/e0/b6/91/e9edcdbf464f623628d5c3aa9de28888c726e270b9a29f2368\n","Successfully built pytreebank\n","Installing collected packages: pytreebank\n","Successfully installed pytreebank-0.2.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"thb55mcORz4m"},"source":["import pytreebank\n","\n","dataset = pytreebank.load_sst('./raw_data')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEMD2wbXSBv2","executionInfo":{"status":"ok","timestamp":1619954496969,"user_tz":-330,"elapsed":1683,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"09b47f76-ec5b-44fd-da8a-3c50122e2d91"},"source":["import os\n","import sys\n","\n","print(sys.path[0])\n","filepath = os.path.join(sys.path[0],'{}.txt')\n","\n","def find_label(label):\n","    if label in (1,2):\n","        return 0\n","    elif label == 3:\n","        return 2\n","    elif label in (4,5):\n","        return 1\n","\n","\n","types = ['train' , 'test', 'dev']\n","\n","for t in types:\n","    with open(filepath.format(t), 'w') as f:\n","        for row in dataset[t]:\n","            label = find_label(row.to_labeled_lines()[0][0]+1)            \n","            f.write(\"{}\\t{}\\n\".format(row.to_labeled_lines()[0][1], label))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehP-GU_rI8oA","executionInfo":{"status":"ok","timestamp":1619954499737,"user_tz":-330,"elapsed":994,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"47445fca-9d09-46e1-e518-a2fbdc405995"},"source":["for t in types:\n","    print(t,\"  \",len(dataset[t]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train    8544\n","test    2210\n","dev    1101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"4-6Ts2XUK6GV","executionInfo":{"status":"ok","timestamp":1619954550332,"user_tz":-330,"elapsed":847,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"c6860b4d-64f2-4117-cfbb-fe48aee39a30"},"source":["import pandas as pd\n","\n","df = pd.read_csv('/content/train.txt', sep='\\t', header=None, names = ['tweets', 'labels'])\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweets</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The Rock is destined to be the 21st Century 's...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The gorgeously elaborate continuation of `` Th...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Singer/composer Bryan Adams contributes a slew...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>You 'd think by now America would have had eno...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Yet the act is still charming here .</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              tweets  labels\n","0  The Rock is destined to be the 21st Century 's...       1\n","1  The gorgeously elaborate continuation of `` Th...       1\n","2  Singer/composer Bryan Adams contributes a slew...       1\n","3  You 'd think by now America would have had eno...       2\n","4               Yet the act is still charming here .       1"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"qxPQcT-CefX1"},"source":["Always look through your dataset to understand it more."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ux6PBenCeUFf","executionInfo":{"status":"ok","timestamp":1619954557293,"user_tz":-330,"elapsed":884,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"cb494f1b-dc50-4a02-99ee-541199612012"},"source":["df.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8544, 2)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ea1fY-pgepmZ","executionInfo":{"status":"ok","timestamp":1619954558846,"user_tz":-330,"elapsed":956,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"25d5d1ab-efa8-4403-f6f4-4792d0274375"},"source":["df.labels.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    3610\n","0    3310\n","2    1624\n","Name: labels, dtype: int64"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"lbFm4S2De_jJ"},"source":["You can use df.labels and df.tweets (name of the column in your dataset) to access the same."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbfHeuRnevOM","executionInfo":{"status":"ok","timestamp":1619954588297,"user_tz":-330,"elapsed":5105,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"a16c27f3-baf1-484d-fad1-0fa983a2a46e"},"source":["import random\n","import torch, torchtext\n","from torchtext.legacy import data\n","\n","SEED = 43\n","torch.manual_seed(SEED)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f5648363030>"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"yq31PnQGfX9z","executionInfo":{"status":"ok","timestamp":1619954591498,"user_tz":-330,"elapsed":966,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"e5d1c459-3305-4afb-fc88-16e582b38491"},"source":["torch.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.8.1+cu101'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"Gbrc_OhEwjaP"},"source":["## Defining Fields\n","\n","\n","Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequential to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."]},{"cell_type":"code","metadata":{"id":"tGD8VmpWe59v"},"source":["Tweet = data.Field(sequential=True, tokenize='spacy', batch_first = True, include_lengths=True)\n","Label = data.LabelField(tokenize='spacy', is_target=True, batch_first=True, sequential=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qrLMqkdSxEn8"},"source":["Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"]},{"cell_type":"code","metadata":{"id":"ynd-UGvFgOyn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619954601121,"user_tz":-330,"elapsed":960,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"fd63c1de-d159-4a12-e6d5-b04b63f8cb5c"},"source":["fields = [('tweets', Tweet),('labels', Label)]\n","fields"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('tweets', <torchtext.legacy.data.field.Field at 0x7f5647360510>),\n"," ('labels', <torchtext.legacy.data.field.LabelField at 0x7f55f1a8f090>)]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"GK1AeBvExkai"},"source":["Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PBAwmh1GxtID","executionInfo":{"status":"ok","timestamp":1619954603495,"user_tz":-330,"elapsed":1030,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"0eab20d9-5106-4135-c3eb-3a71882542a7"},"source":["df.tweets[0], df.labels[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\",\n"," 1)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mu56uzV9yRXh","executionInfo":{"status":"ok","timestamp":1619954607706,"user_tz":-330,"elapsed":915,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"6f5af6b8-a77d-4b5d-d0f9-54a1e0780a21"},"source":["df.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8544"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U_mtCz9JxQlX","executionInfo":{"status":"ok","timestamp":1619955012370,"user_tz":-330,"elapsed":403814,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"820ffe14-54a8-449c-d4be-8c068349d8ac"},"source":["%%time\n","example = [data.Example.fromlist([df.tweets[i], df.labels[i]], fields) for i in range(df.shape[0])]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 6min 38s, sys: 5.06 s, total: 6min 43s\n","Wall time: 6min 43s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0KFRR7KzGO3","executionInfo":{"status":"ok","timestamp":1619955280121,"user_tz":-330,"elapsed":989,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"bcc0b2c7-1999-4381-fb08-0d7c72fb0571"},"source":["example[:5][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torchtext.legacy.data.example.Example at 0x7f55f1a8cdd0>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"IXNx58fOy4S9"},"source":["## Creating dataset"]},{"cell_type":"code","metadata":{"id":"VWS6qJUwyUwV"},"source":["## Approach 1\n","##twitterDataset = data.TabularDataset(path='/content/tweets.csv', format=\"CSV\", fields = fields, skip_header=True)\n","\n","## Approach 2\n","twitterDataset = data.Dataset(example, fields)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CBIiWki8znZ4"},"source":["## Split the dataset"]},{"cell_type":"code","metadata":{"id":"8zF7Ky27y8bk"},"source":["train, valid = twitterDataset.split(split_ratio=[0.85,0.15], random_state= random.seed(SEED))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSz0BNqrz5Fv","executionInfo":{"status":"ok","timestamp":1619955295107,"user_tz":-330,"elapsed":594,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"66d08161-ed69-45ef-c0f6-bf2d552cf8ac"},"source":["len(train), len(valid)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7262, 1282)"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"flg7hQLTz_5O","executionInfo":{"status":"ok","timestamp":1619955295721,"user_tz":-330,"elapsed":924,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"452556ae-e5d0-46f8-aa33-888b6eeede5e"},"source":["vars(train.examples[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'labels': 1,\n"," 'tweets': ['Asks',\n","  'what',\n","  'truth',\n","  'can',\n","  'be',\n","  'discerned',\n","  'from',\n","  'non',\n","  '-',\n","  'firsthand',\n","  'experience',\n","  ',',\n","  'and',\n","  'specifically',\n","  'questions',\n","  'cinema',\n","  \"'s\",\n","  'capability',\n","  'for',\n","  'recording',\n","  'truth',\n","  '.']}"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"XaPP79e00J8v"},"source":["## Building Vocabulary\n","\n","At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n","\n","Let’s limit the vocabulary to a maximum of 5000 words in our training set:"]},{"cell_type":"code","metadata":{"id":"3TlWXw7_0Df0"},"source":["Tweet.build_vocab(train)\n","Label.build_vocab(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZ4LimkS0ekq"},"source":["By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkSysm8H0ZcY","executionInfo":{"status":"ok","timestamp":1619955301173,"user_tz":-330,"elapsed":1109,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"64bc91bb-30b1-48b7-9ce6-eb118da8c80e"},"source":["print('Size of input vocab : ', len(Tweet.vocab))\n","print('Size of label vocab : ', len(Label.vocab))\n","print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n","print('Labels : ', Label.vocab.stoi)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Size of input vocab :  15741\n","Size of label vocab :  3\n","Top 10 words appreared repeatedly : [('.', 6832), (',', 6060), ('the', 5189), ('of', 3767), ('and', 3756), ('a', 3755), ('to', 2588), ('-', 2342), (\"'s\", 2158), ('is', 2151)]\n","Labels :  defaultdict(None, {1: 0, 0: 1, 2: 2})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qItn_Ak0iL9","executionInfo":{"status":"ok","timestamp":1619955306067,"user_tz":-330,"elapsed":1317,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"79bd5de4-e48b-41a3-e2f1-ba8a19d34a52"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"LDUmadqj09ji"},"source":["BATCH_SIZE=32\n","\n","train_iterator, valid_iterator = data.BucketIterator.splits((train, valid),\n","                           batch_size = BATCH_SIZE,\n","                           sort_key = lambda x: len(x.tweets),\n","                           sort_within_batch=True,\n","                           device = device\n","                           )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vgBreVW12X3"},"source":["Save the vocabulary"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5JWdLii2FHV","executionInfo":{"status":"ok","timestamp":1619955312154,"user_tz":-330,"elapsed":896,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"f7e08349-43e9-4ae8-f2a0-ddcfdd28a254"},"source":["Tweet.vocab.stoi"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f55ee856a50>>,\n","            {'<unk>': 0,\n","             '<pad>': 1,\n","             '.': 2,\n","             ',': 3,\n","             'the': 4,\n","             'of': 5,\n","             'and': 6,\n","             'a': 7,\n","             'to': 8,\n","             '-': 9,\n","             \"'s\": 10,\n","             'is': 11,\n","             'that': 12,\n","             'in': 13,\n","             'it': 14,\n","             'The': 15,\n","             'as': 16,\n","             'film': 17,\n","             'with': 18,\n","             'but': 19,\n","             'movie': 20,\n","             'for': 21,\n","             'its': 22,\n","             'A': 23,\n","             '`': 24,\n","             'an': 25,\n","             'you': 26,\n","             'this': 27,\n","             'be': 28,\n","             \"n't\": 29,\n","             'It': 30,\n","             '...': 31,\n","             'on': 32,\n","             \"'\": 33,\n","             'not': 34,\n","             '--': 35,\n","             'by': 36,\n","             'has': 37,\n","             'about': 38,\n","             'are': 39,\n","             'more': 40,\n","             'one': 41,\n","             'from': 42,\n","             'than': 43,\n","             'at': 44,\n","             'have': 45,\n","             'like': 46,\n","             'I': 47,\n","             'all': 48,\n","             'his': 49,\n","             'so': 50,\n","             'or': 51,\n","             '(': 52,\n","             ')': 53,\n","             'out': 54,\n","             'story': 55,\n","             'up': 56,\n","             'too': 57,\n","             'into': 58,\n","             'who': 59,\n","             'does': 60,\n","             'good': 61,\n","             'most': 62,\n","             \"''\": 63,\n","             'just': 64,\n","             'comedy': 65,\n","             'will': 66,\n","             'This': 67,\n","             'can': 68,\n","             'if': 69,\n","             'much': 70,\n","             'characters': 71,\n","             'no': 72,\n","             'time': 73,\n","             'their': 74,\n","             'funny': 75,\n","             'what': 76,\n","             'even': 77,\n","             'some': 78,\n","             'only': 79,\n","             'well': 80,\n","             'way': 81,\n","             'little': 82,\n","             'been': 83,\n","             'he': 84,\n","             'very': 85,\n","             'which': 86,\n","             'would': 87,\n","             'make': 88,\n","             'your': 89,\n","             'An': 90,\n","             'life': 91,\n","             'may': 92,\n","             'there': 93,\n","             'director': 94,\n","             'do': 95,\n","             'never': 96,\n","             'work': 97,\n","             'enough': 98,\n","             'they': 99,\n","             'us': 100,\n","             'any': 101,\n","             'makes': 102,\n","             '?': 103,\n","             'best': 104,\n","             'was': 105,\n","             'made': 106,\n","             'bad': 107,\n","             'movies': 108,\n","             'we': 109,\n","             'If': 110,\n","             'off': 111,\n","             'drama': 112,\n","             'There': 113,\n","             'through': 114,\n","             ':': 115,\n","             'action': 116,\n","             'could': 117,\n","             'love': 118,\n","             'really': 119,\n","             'something': 120,\n","             'own': 121,\n","             'see': 122,\n","             'many': 123,\n","             'first': 124,\n","             'her': 125,\n","             'other': 126,\n","             'plot': 127,\n","             'should': 128,\n","             'how': 129,\n","             'look': 130,\n","             'What': 131,\n","             'when': 132,\n","             'better': 133,\n","             'without': 134,\n","             \"'re\": 135,\n","             'over': 136,\n","             'people': 137,\n","             'performances': 138,\n","             'two': 139,\n","             'films': 140,\n","             'old': 141,\n","             'still': 142,\n","             'also': 143,\n","             'character': 144,\n","             'long': 145,\n","             'audience': 146,\n","             'between': 147,\n","             'ever': 148,\n","             'script': 149,\n","             'feel': 150,\n","             'might': 151,\n","             'them': 152,\n","             'those': 153,\n","             'cast': 154,\n","             'being': 155,\n","             'both': 156,\n","             'fun': 157,\n","             'here': 158,\n","             'new': 159,\n","             'sense': 160,\n","             'great': 161,\n","             'humor': 162,\n","             'performance': 163,\n","             'real': 164,\n","             'self': 165,\n","             'every': 166,\n","             'get': 167,\n","             'nothing': 168,\n","             'down': 169,\n","             'hard': 170,\n","             'In': 171,\n","             'big': 172,\n","             ';': 173,\n","             'end': 174,\n","             'As': 175,\n","             'But': 176,\n","             'picture': 177,\n","             'thriller': 178,\n","             'You': 179,\n","             'few': 180,\n","             '!': 181,\n","             'because': 182,\n","             'often': 183,\n","             'screen': 184,\n","             'Hollywood': 185,\n","             'less': 186,\n","             'seems': 187,\n","             'tale': 188,\n","             'another': 189,\n","             'documentary': 190,\n","             'heart': 191,\n","             'kind': 192,\n","             'were': 193,\n","             'minutes': 194,\n","             'such': 195,\n","             'One': 196,\n","             'almost': 197,\n","             'entertaining': 198,\n","             've': 199,\n","             'world': 200,\n","             'yet': 201,\n","             \"'ll\": 202,\n","             'family': 203,\n","             'far': 204,\n","             'rather': 205,\n","             'while': 206,\n","             'before': 207,\n","             'lot': 208,\n","             'quite': 209,\n","             'right': 210,\n","             'these': 211,\n","             'American': 212,\n","             'comes': 213,\n","             'man': 214,\n","             'after': 215,\n","             'back': 216,\n","             'feels': 217,\n","             'seen': 218,\n","             'acting': 219,\n","             'find': 220,\n","             'go': 221,\n","             'interesting': 222,\n","             'ca': 223,\n","             'watch': 224,\n","             'year': 225,\n","             'come': 226,\n","             'had': 227,\n","             'itself': 228,\n","             'me': 229,\n","             'moments': 230,\n","             'things': 231,\n","             'worth': 232,\n","             'While': 233,\n","             'original': 234,\n","             'take': 235,\n","             'Like': 236,\n","             'actors': 237,\n","             'dialogue': 238,\n","             'gets': 239,\n","             'material': 240,\n","             'times': 241,\n","             'romantic': 242,\n","             'then': 243,\n","             'thing': 244,\n","             'young': 245,\n","             'emotional': 246,\n","             'making': 247,\n","             'where': 248,\n","             'bit': 249,\n","             'human': 250,\n","             'style': 251,\n","             'compelling': 252,\n","             'ultimately': 253,\n","             'want': 254,\n","             'watching': 255,\n","             'works': 256,\n","             '/': 257,\n","             'scenes': 258,\n","             'subject': 259,\n","             'think': 260,\n","             'again': 261,\n","             'full': 262,\n","             'point': 263,\n","             'seem': 264,\n","             'though': 265,\n","             'years': 266,\n","             'least': 267,\n","             'music': 268,\n","             'once': 269,\n","             'For': 270,\n","             'same': 271,\n","             'did': 272,\n","             'give': 273,\n","             'With': 274,\n","             'actually': 275,\n","             'comic': 276,\n","             'dull': 277,\n","             'fascinating': 278,\n","             'flick': 279,\n","             'keep': 280,\n","             'our': 281,\n","             'show': 282,\n","             'cinema': 283,\n","             'gives': 284,\n","             'high': 285,\n","             'moving': 286,\n","             'Not': 287,\n","             'him': 288,\n","             'need': 289,\n","             'piece': 290,\n","             'takes': 291,\n","             'dark': 292,\n","             'going': 293,\n","             'history': 294,\n","             'matter': 295,\n","             'special': 296,\n","             'women': 297,\n","             'anything': 298,\n","             'art': 299,\n","             'coming': 300,\n","             'feature': 301,\n","             'part': 302,\n","             'say': 303,\n","             'she': 304,\n","             'whole': 305,\n","             'away': 306,\n","             'half': 307,\n","             'low': 308,\n","             'manages': 309,\n","             'direction': 310,\n","             'genre': 311,\n","             'kids': 312,\n","             'know': 313,\n","             'together': 314,\n","             'whose': 315,\n","             'always': 316,\n","             'engaging': 317,\n","             'entertainment': 318,\n","             'hour': 319,\n","             'narrative': 320,\n","             'screenplay': 321,\n","             'silly': 322,\n","             'video': 323,\n","             'writer': 324,\n","             'Mr.': 325,\n","             'fans': 326,\n","             'last': 327,\n","             'pretty': 328,\n","             'short': 329,\n","             'children': 330,\n","             'done': 331,\n","             'nearly': 332,\n","             'true': 333,\n","             'worst': 334,\n","             'All': 335,\n","             'age': 336,\n","             'anyone': 337,\n","             'care': 338,\n","             'cinematic': 339,\n","             'clever': 340,\n","             'goes': 341,\n","             'looking': 342,\n","             'series': 343,\n","             'since': 344,\n","             'smart': 345,\n","             'three': 346,\n","             'visual': 347,\n","             'Though': 348,\n","             'around': 349,\n","             'charm': 350,\n","             'face': 351,\n","             'idea': 352,\n","             'probably': 353,\n","             'simply': 354,\n","             'strong': 355,\n","             'title': 356,\n","             \"'d\": 357,\n","             'New': 358,\n","             'experience': 359,\n","             'familiar': 360,\n","             'set': 361,\n","             'sometimes': 362,\n","             'sweet': 363,\n","             'under': 364,\n","             'And': 365,\n","             'French': 366,\n","             'That': 367,\n","             'amusing': 368,\n","             'beautiful': 369,\n","             'else': 370,\n","             'enjoyable': 371,\n","             'exercise': 372,\n","             'filmmakers': 373,\n","             'filmmaking': 374,\n","             'horror': 375,\n","             'laughs': 376,\n","             'place': 377,\n","             'powerful': 378,\n","             'study': 379,\n","             'thought': 380,\n","             'wo': 381,\n","             'certainly': 382,\n","             'dramatic': 383,\n","             'easy': 384,\n","             'effects': 385,\n","             'predictable': 386,\n","             'premise': 387,\n","             'star': 388,\n","             'why': 389,\n","             'Despite': 390,\n","             'Even': 391,\n","             'becomes': 392,\n","             'solid': 393,\n","             'turns': 394,\n","             'energy': 395,\n","             'especially': 396,\n","             'fact': 397,\n","             'feeling': 398,\n","             'light': 399,\n","             'must': 400,\n","             'offers': 401,\n","             'portrait': 402,\n","             'war': 403,\n","             'TV': 404,\n","             'believe': 405,\n","             'each': 406,\n","             'effort': 407,\n","             'likely': 408,\n","             'lives': 409,\n","             'modern': 410,\n","             'plays': 411,\n","             'become': 412,\n","             'either': 413,\n","             'enjoy': 414,\n","             'fresh': 415,\n","             'm': 416,\n","             'mind': 417,\n","             'my': 418,\n","             'opera': 419,\n","             'power': 420,\n","             'quirky': 421,\n","             'reason': 422,\n","             'serious': 423,\n","             'social': 424,\n","             'summer': 425,\n","             'surprisingly': 426,\n","             'theater': 427,\n","             'trying': 428,\n","             'version': 429,\n","             'John': 430,\n","             'already': 431,\n","             'debut': 432,\n","             'ending': 433,\n","             'instead': 434,\n","             'mess': 435,\n","             'romance': 436,\n","             'sort': 437,\n","             'sure': 438,\n","             'tone': 439,\n","             'truly': 440,\n","             'wit': 441,\n","             'At': 442,\n","             'No': 443,\n","             'book': 444,\n","             'charming': 445,\n","             'classic': 446,\n","             'culture': 447,\n","             'everything': 448,\n","             'hilarious': 449,\n","             'himself': 450,\n","             'interest': 451,\n","             'lack': 452,\n","             'lacks': 453,\n","             'laugh': 454,\n","             'line': 455,\n","             'obvious': 456,\n","             'play': 457,\n","             'political': 458,\n","             'turn': 459,\n","             'About': 460,\n","             'Just': 461,\n","             'My': 462,\n","             'animation': 463,\n","             'completely': 464,\n","             'everyone': 465,\n","             'fine': 466,\n","             'looks': 467,\n","             'message': 468,\n","             'problem': 469,\n","             'school': 470,\n","             'small': 471,\n","             'Director': 472,\n","             'acted': 473,\n","             'actor': 474,\n","             'beautifully': 475,\n","             'beyond': 476,\n","             'ideas': 477,\n","             'now': 478,\n","             'production': 479,\n","             'rare': 480,\n","             'shows': 481,\n","             'spirit': 482,\n","             'teen': 483,\n","             'Although': 484,\n","             'By': 485,\n","             'Its': 486,\n","             'cold': 487,\n","             'complex': 488,\n","             'difficult': 489,\n","             'directed': 490,\n","             'fails': 491,\n","             'found': 492,\n","             'gags': 493,\n","             'images': 494,\n","             'intelligence': 495,\n","             'left': 496,\n","             'live': 497,\n","             'neither': 498,\n","             'particularly': 499,\n","             'perfect': 500,\n","             'proves': 501,\n","             'stuff': 502,\n","             'viewers': 503,\n","             'When': 504,\n","             'adventure': 505,\n","             'along': 506,\n","             'boring': 507,\n","             'day': 508,\n","             'different': 509,\n","             'filmmaker': 510,\n","             'jokes': 511,\n","             'melodrama': 512,\n","             'middle': 513,\n","             'sad': 514,\n","             'seeing': 515,\n","             'stand': 516,\n","             'stories': 517,\n","             'storytelling': 518,\n","             'top': 519,\n","             'touching': 520,\n","             'ways': 521,\n","             'woman': 522,\n","             'Man': 523,\n","             'We': 524,\n","             'above': 525,\n","             'against': 526,\n","             'audiences': 527,\n","             'deeply': 528,\n","             'despite': 529,\n","             'easily': 530,\n","             'flat': 531,\n","             'home': 532,\n","             'intelligent': 533,\n","             'leave': 534,\n","             'mostly': 535,\n","             'nor': 536,\n","             'rich': 537,\n","             'ride': 538,\n","             'sex': 539,\n","             'shot': 540,\n","             'soap': 541,\n","             'thin': 542,\n","             'tries': 543,\n","             'written': 544,\n","             'Disney': 545,\n","             'He': 546,\n","             'Love': 547,\n","             'Michael': 548,\n","             'act': 549,\n","             'attempt': 550,\n","             'brilliant': 551,\n","             'camera': 552,\n","             'case': 553,\n","             'cliches': 554,\n","             'concept': 555,\n","             'death': 556,\n","             'getting': 557,\n","             'heavy': 558,\n","             'honest': 559,\n","             'hours': 560,\n","             'impossible': 561,\n","             'level': 562,\n","             'mystery': 563,\n","             'next': 564,\n","             'period': 565,\n","             'remains': 566,\n","             'scene': 567,\n","             'surprising': 568,\n","             'To': 569,\n","             'appeal': 570,\n","             'bland': 571,\n","             'budget': 572,\n","             'class': 573,\n","             'deep': 574,\n","             'dumb': 575,\n","             'during': 576,\n","             'head': 577,\n","             'job': 578,\n","             'men': 579,\n","             'mood': 580,\n","             'put': 581,\n","             'remarkable': 582,\n","             'role': 583,\n","             'suspense': 584,\n","             'tragedy': 585,\n","             'viewer': 586,\n","             'writing': 587,\n","             'Has': 588,\n","             'Oscar': 589,\n","             'They': 590,\n","             'Time': 591,\n","             'Too': 592,\n","             'black': 593,\n","             'contrived': 594,\n","             'crime': 595,\n","             'delivers': 596,\n","             'elements': 597,\n","             'emotionally': 598,\n","             'ends': 599,\n","             'falls': 600,\n","             'finally': 601,\n","             'game': 602,\n","             'given': 603,\n","             'having': 604,\n","             'hero': 605,\n","             'imagination': 606,\n","             'intriguing': 607,\n","             'lost': 608,\n","             'loud': 609,\n","             'perfectly': 610,\n","             'pleasure': 611,\n","             'plenty': 612,\n","             'possible': 613,\n","             'reality': 614,\n","             'recent': 615,\n","             'run': 616,\n","             'running': 617,\n","             'satisfying': 618,\n","             'sequel': 619,\n","             'sequences': 620,\n","             'side': 621,\n","             'simple': 622,\n","             'slow': 623,\n","             'straight': 624,\n","             'terrific': 625,\n","             'themselves': 626,\n","             'violence': 627,\n","             'wrong': 628,\n","             '*': 629,\n","             'Big': 630,\n","             'David': 631,\n","             'De': 632,\n","             'II': 633,\n","             'Is': 634,\n","             'Some': 635,\n","             'able': 636,\n","             'change': 637,\n","             'close': 638,\n","             'days': 639,\n","             'eye': 640,\n","             'eyes': 641,\n","             'formula': 642,\n","             'girl': 643,\n","             'inside': 644,\n","             'leaves': 645,\n","             'novel': 646,\n","             'occasionally': 647,\n","             'otherwise': 648,\n","             'passion': 649,\n","             'playing': 650,\n","             'pretentious': 651,\n","             'psychological': 652,\n","             'satire': 653,\n","             'somewhat': 654,\n","             'strange': 655,\n","             'told': 656,\n","             'truth': 657,\n","             'approach': 658,\n","             'artist': 659,\n","             'attention': 660,\n","             'barely': 661,\n","             'boy': 662,\n","             'bring': 663,\n","             'captures': 664,\n","             'cheap': 665,\n","             'crafted': 666,\n","             'creative': 667,\n","             'entirely': 668,\n","             'events': 669,\n","             'exactly': 670,\n","             'execution': 671,\n","             'fashioned': 672,\n","             'got': 673,\n","             'journey': 674,\n","             'keeps': 675,\n","             'latest': 676,\n","             'memorable': 677,\n","             'offer': 678,\n","             'past': 679,\n","             'thoughtful': 680,\n","             'tired': 681,\n","             'uses': 682,\n","             'usual': 683,\n","             'wonderful': 684,\n","             'After': 685,\n","             'B': 686,\n","             'Robert': 687,\n","             'Spielberg': 688,\n","             'adults': 689,\n","             'clear': 690,\n","             'comedies': 691,\n","             'creepy': 692,\n","             'cut': 693,\n","             'epic': 694,\n","             'fairly': 695,\n","             'fantasy': 696,\n","             'final': 697,\n","             'gone': 698,\n","             'hand': 699,\n","             'hit': 700,\n","             'parents': 701,\n","             'personal': 702,\n","             'relationships': 703,\n","             'result': 704,\n","             'scary': 705,\n","             'single': 706,\n","             'stupid': 707,\n","             'talent': 708,\n","             'taste': 709,\n","             'tell': 710,\n","             'vision': 711,\n","             'Grant': 712,\n","             'Home': 713,\n","             'More': 714,\n","             'So': 715,\n","             'air': 716,\n","             'appealing': 717,\n","             'certain': 718,\n","             'co': 719,\n","             'delightful': 720,\n","             'effective': 721,\n","             'engrossing': 722,\n","             'examination': 723,\n","             'expect': 724,\n","             'female': 725,\n","             'gentle': 726,\n","             'guys': 727,\n","             'help': 728,\n","             'historical': 729,\n","             'impressive': 730,\n","             'masterpiece': 731,\n","             'moment': 732,\n","             'needs': 733,\n","             'odd': 734,\n","             'pace': 735,\n","             'pictures': 736,\n","             'project': 737,\n","             'quality': 738,\n","             'saw': 739,\n","             'sit': 740,\n","             'start': 741,\n","             'tedious': 742,\n","             'ugly': 743,\n","             'unsettling': 744,\n","             'used': 745,\n","             'view': 746,\n","             'whether': 747,\n","             'working': 748,\n","             'America': 749,\n","             'Do': 750,\n","             'Every': 751,\n","             'York': 752,\n","             'animated': 753,\n","             'begins': 754,\n","             'called': 755,\n","             'date': 756,\n","             'decent': 757,\n","             'emotions': 758,\n","             'era': 759,\n","             'fi': 760,\n","             'gay': 761,\n","             'gorgeous': 762,\n","             'huge': 763,\n","             'impact': 764,\n","             'important': 765,\n","             'issues': 766,\n","             'knows': 767,\n","             'lead': 768,\n","             'magic': 769,\n","             'mean': 770,\n","             'mediocre': 771,\n","             'merely': 772,\n","             'monster': 773,\n","             'moral': 774,\n","             'nature': 775,\n","             'none': 776,\n","             'overall': 777,\n","             'plain': 778,\n","             'pop': 779,\n","             'potential': 780,\n","             'pure': 781,\n","             'road': 782,\n","             'sci': 783,\n","             'sentimental': 784,\n","             'success': 785,\n","             'surprise': 786,\n","             'taking': 787,\n","             'thoroughly': 788,\n","             'try': 789,\n","             'understand': 790,\n","             'urban': 791,\n","             'watchable': 792,\n","             'word': 793,\n","             'worse': 794,\n","             'Allen': 795,\n","             'Full': 796,\n","             'How': 797,\n","             'Nothing': 798,\n","             'Thing': 799,\n","             'ability': 800,\n","             'adaptation': 801,\n","             'amount': 802,\n","             'awful': 803,\n","             'based': 804,\n","             'battle': 805,\n","             'convincing': 806,\n","             'cool': 807,\n","             'creates': 808,\n","             'depth': 809,\n","             'doubt': 810,\n","             'ensemble': 811,\n","             'felt': 812,\n","             'finds': 813,\n","             'four': 814,\n","             'hope': 815,\n","             'imagine': 816,\n","             'insight': 817,\n","             'inspired': 818,\n","             'key': 819,\n","             'lacking': 820,\n","             'melodramatic': 821,\n","             'near': 822,\n","             'nice': 823,\n","             'others': 824,\n","             'problems': 825,\n","             'process': 826,\n","             'puts': 827,\n","             'quiet': 828,\n","             'relationship': 829,\n","             'rest': 830,\n","             'sexual': 831,\n","             'sharp': 832,\n","             'situation': 833,\n","             'stars': 834,\n","             'subtle': 835,\n","             'terms': 836,\n","             'thinking': 837,\n","             'throughout': 838,\n","             'touch': 839,\n","             'upon': 840,\n","             'use': 841,\n","             'viewing': 842,\n","             'visually': 843,\n","             'warm': 844,\n","             'winning': 845,\n","             'words': 846,\n","             '2': 847,\n","             'Blade': 848,\n","             'Day': 849,\n","             'Movie': 850,\n","             'War': 851,\n","             'Williams': 852,\n","             'across': 853,\n","             'ambitious': 854,\n","             'business': 855,\n","             'casting': 856,\n","             'college': 857,\n","             'contemporary': 858,\n","             'create': 859,\n","             'dead': 860,\n","             'definitely': 861,\n","             'dry': 862,\n","             'entire': 863,\n","             'episode': 864,\n","             'equally': 865,\n","             'excellent': 866,\n","             'fast': 867,\n","             'five': 868,\n","             'flaws': 869,\n","             'force': 870,\n","             'form': 871,\n","             'future': 872,\n","             'goofy': 873,\n","             'hold': 874,\n","             'however': 875,\n","             'joke': 876,\n","             'kid': 877,\n","             'length': 878,\n","             'liked': 879,\n","             'manner': 880,\n","             'meaning': 881,\n","             'mildly': 882,\n","             'mother': 883,\n","             'name': 884,\n","             'open': 885,\n","             'points': 886,\n","             'previous': 887,\n","             'promise': 888,\n","             'provocative': 889,\n","             'quickly': 890,\n","             'rarely': 891,\n","             'routine': 892,\n","             'second': 893,\n","             'several': 894,\n","             'sincere': 895,\n","             'starts': 896,\n","             'stylish': 897,\n","             'supposed': 898,\n","             'sustain': 899,\n","             'trip': 900,\n","             'until': 901,\n","             'virtually': 902,\n","             'white': 903,\n","             '2002': 904,\n","             '90': 905,\n","             'British': 906,\n","             'Feels': 907,\n","             'George': 908,\n","             'Jackson': 909,\n","             'Much': 910,\n","             'Murphy': 911,\n","             'Sandler': 912,\n","             'Solondz': 913,\n","             'Tom': 914,\n","             'Watching': 915,\n","             'Ya': 916,\n","             'behind': 917,\n","             'clichés': 918,\n","             'constructed': 919,\n","             'couple': 920,\n","             'created': 921,\n","             'cultural': 922,\n","             'cute': 923,\n","             'depressing': 924,\n","             'extreme': 925,\n","             'fan': 926,\n","             'fiction': 927,\n","             'flicks': 928,\n","             'forgettable': 929,\n","             'fully': 930,\n","             'gross': 931,\n","             'hip': 932,\n","             'house': 933,\n","             'interested': 934,\n","             'intimate': 935,\n","             'involved': 936,\n","             'leads': 937,\n","             'living': 938,\n","             'lovely': 939,\n","             'major': 940,\n","             'motion': 941,\n","             'pacing': 942,\n","             'painful': 943,\n","             'pieces': 944,\n","             'pleasant': 945,\n","             'provides': 946,\n","             'sensitive': 947,\n","             'seriously': 948,\n","             'sexy': 949,\n","             'slight': 950,\n","             'slightly': 951,\n","             'someone': 952,\n","             'sound': 953,\n","             'successful': 954,\n","             'surprises': 955,\n","             'taken': 956,\n","             'talented': 957,\n","             'tragic': 958,\n","             'treatment': 959,\n","             'unfunny': 960,\n","             'unique': 961,\n","             'utterly': 962,\n","             'welcome': 963,\n","             'worthy': 964,\n","             'Bullock': 965,\n","             'Chan': 966,\n","             'Crush': 967,\n","             'Moore': 968,\n","             'Ms.': 969,\n","             'Never': 970,\n","             'Niro': 971,\n","             'Peter': 972,\n","             'Steven': 973,\n","             'actress': 974,\n","             'add': 975,\n","             'apart': 976,\n","             'attempts': 977,\n","             'badly': 978,\n","             'brain': 979,\n","             'brings': 980,\n","             'cartoon': 981,\n","             'cliché': 982,\n","             'complete': 983,\n","             'core': 984,\n","             'deserves': 985,\n","             'dog': 986,\n","             'earnest': 987,\n","             'edge': 988,\n","             'effect': 989,\n","             'except': 990,\n","             'flawed': 991,\n","             'free': 992,\n","             'genuine': 993,\n","             'gripping': 994,\n","             'hardly': 995,\n","             'highly': 996,\n","             'insightful': 997,\n","             'intended': 998,\n","             'lets': 999,\n","             ...})"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"ew78rLv11vbP"},"source":["import os, pickle\n","\n","with open('tokenizer.pkl', 'wb') as t:\n","    pickle.dump(Tweet.vocab.stoi, t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZNnKBYk32OW-"},"source":["## Define the model\n","\n","We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n","\n","In this model we create three layers. \n","1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n","2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n","3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."]},{"cell_type":"code","metadata":{"id":"7g_d5wc12Hpg"},"source":["import torch .nn as nn\n","import torch.nn.functional as F\n","\n","class sentimentClassifier(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, dropout, output_dim):\n","        super().__init__()\n","\n","        # Embedding layer\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","\n","        #LSTM layer\n","        self.encoder = nn.LSTM(embedding_dim,\n","                            hidden_dim,\n","                            num_layers=n_layers,\n","                            dropout=dropout,\n","                            batch_first=True)\n","        # try using nn.GRU or nn.RNN here and compare their performances\n","        # try bidirectional and compare their performances\n","\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","    \n","    def forward(self, text, text_lengths):\n","        # text = [batch_size, text_length] \n","        embedded = self.embedding(text)\n","\n","        # embedded = [batch_size, text_length, embeddding_dim]\n","\n","        # packed sequence\n","        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,\n","                                                            text_lengths.cpu(),\n","                                                            batch_first=True)\n","        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n","        # hidden = [batch_size, num_layers*num_directions, hidden_dim]\n","        # cell = [batch_size, num_layers*num_directions, hidden_dim]\n","\n","        dense_outputs = self.fc(hidden)\n","\n","        # Activation function Softmax\n","        output = F.softmax(dense_outputs[0], dim=1)\n","\n","        return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iciw10EO9_jm","executionInfo":{"status":"ok","timestamp":1619955803806,"user_tz":-330,"elapsed":1127,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"0801e3a7-b085-44f8-c510-23a1c0db9f6b"},"source":["# Define Hyperparameters\n","\n","VOCAB_SIZE = len(Tweet.vocab)\n","EMBEDDING_DIM = 300\n","HIDDEN_DIM = 200\n","OUTPUT_DIM = 3\n","NUM_LAYERS = 3\n","DROPOUT = 0.15\n","\n","model = sentimentClassifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, OUTPUT_DIM)\n","model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["sentimentClassifier(\n","  (embedding): Embedding(15741, 300)\n","  (encoder): LSTM(300, 200, num_layers=3, batch_first=True, dropout=0.15)\n","  (fc): Linear(in_features=200, out_features=3, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02ci_711-z9c","executionInfo":{"status":"ok","timestamp":1619955803807,"user_tz":-330,"elapsed":550,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"470320c5-b65b-48d1-f9ae-4f515623ff19"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","count_parameters(model)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5767703"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"id":"XWez4ssy_TG5"},"source":["## Model Training and Evaluation"]},{"cell_type":"markdown","metadata":{"id":"U24BHhr8_ZDA"},"source":["### Optimizer and Loss Function"]},{"cell_type":"code","metadata":{"id":"vbdWAGKC_N_B"},"source":["import torch.optim as optim\n","\n","optimizer = optim.Adam(model.parameters(), lr=2e-4)\n","criterion = nn.CrossEntropyLoss()\n","\n","# accuracy metric\n","def binary_accuracy(pred,y):\n","    _, predictions = torch.max(pred, 1)\n","    \n","    correct = (predictions == y).float()\n","    accuracy = correct.sum()/len(correct)\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pIvzsE00y-R"},"source":["## Augmentations"]},{"cell_type":"markdown","metadata":{"id":"xYZC_dN81PAX"},"source":["### Random Deletion\n","As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."]},{"cell_type":"code","metadata":{"id":"wCxxWOCd12X7"},"source":["def random_deletion(words, p=0.5): \n","    if len(words) == 1: # return if single word\n","        return words\n","    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n","    if len(remaining) == 0: # if not left, sample a random word\n","        return words\n","    else:\n","        remaning_pad_len = len(words) - len(remaining)\n","        pads = [1]*remaning_pad_len\n","        remaining = torch.cat([torch.tensor(remaining), torch.tensor(pads)], dim=0)\n","        return remaining"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHeDYmLq1RUf"},"source":["### Random Swap\n","The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."]},{"cell_type":"code","metadata":{"id":"m4WTToG-13VH"},"source":["def random_swap(sentence, n=5): \n","    length = range(len(sentence)) \n","    for _ in range(n):\n","        idx1, idx2 = random.sample(length, 2)\n","        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n","    return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-03QJnG1Uvg"},"source":["### Random Insertion\n","A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:"]},{"cell_type":"code","metadata":{"id":"x_FkMeHG1y-h"},"source":["# not used here\n","\n","def random_insertion(sentence, n): \n","    words = remove_stopwords(sentence) \n","    for _ in range(n):\n","        new_synonym = get_synonyms(random.choice(words))\n","        sentence.insert(randrange(len(sentence)+1), new_synonym) \n","    return sentence\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqvDXvZW1ttR"},"source":["### Back Translation\n","\n","Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "]},{"cell_type":"code","metadata":{"id":"D60HAZUz1viN"},"source":["# not used here\n","\n","import random\n","import googletrans\n","import googletrans.Translator\n","\n","translator = Translator()\n","sentence = ['The dog slept on the rug']\n","\n","available_langs = list(googletrans.LANGUAGES.keys()) \n","trans_lang = random.choice(available_langs) \n","print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n","\n","translations = translator.translate(sentence, dest=trans_lang) \n","t_text = [t.text for t in translations]\n","print(t_text)\n","\n","translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n","en_text = [t.text for t in translations_en_random]\n","print(en_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GywiG57J1__b"},"source":["## Apply Augmentations"]},{"cell_type":"code","metadata":{"id":"8KiRP9ZY1_we"},"source":["def augmentation(sentence, dropout=0.3):\n","    probability = random.random()\n","    if probability > 0.3:\n","        n = random.randint(5, 9)\n","        sentence = random_swap(sentence, n)\n","        sentence = random_deletion(sentence, dropout)\n","        return sentence\n","    else:\n","        return sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWeK0ZcHAGjT"},"source":["model = model.to(device)\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AB3pjedoANWL"},"source":["def train(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_accuracy = 0\n","\n","    model.train()\n","\n","    for batch in iterator:\n","        optimizer.zero_grad()\n","\n","        tweet, tweet_lengths = batch.tweets\n","\n","        # apply augmentation\n","        list_of_tweets = [augmentation(t.cpu(), dropout=0.3)for t in tweet]\n","        tweet = torch.stack(list_of_tweets).long().to(device)\n","\n","        prediction = model(tweet, tweet_lengths).squeeze()\n","\n","        # compute the loss\n","        loss = criterion(prediction, batch.labels)\n","\n","        # compute the binary accuracy\n","        accuracy = binary_accuracy(prediction, batch.labels)\n","\n","        # backprops the loss and compute the gradients\n","        loss.backward()\n","\n","        # update the weights\n","        optimizer.step()\n","\n","        # store loss and accuracy\n","        epoch_loss += loss.item()\n","        epoch_accuracy += accuracy.item()\n","\n","    len_iterator = len(iterator)\n","    return epoch_loss/len_iterator , epoch_accuracy/len_iterator"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wpxUwpdPCFN2"},"source":["def evaluate(model, iterator, optimizer, criterion):\n","    epoch_loss = 0\n","    epoch_accuracy = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for batch in iterator:\n","\n","            tweet, tweet_lengths = batch.tweets\n","            prediction = model(tweet, tweet_lengths).squeeze()\n","\n","            # compute the loss\n","            loss = criterion(prediction, batch.labels)\n","\n","            # compute the binary accuracy\n","            accuracy = binary_accuracy(prediction, batch.labels)\n","\n","            # store loss and accuracy\n","            epoch_loss += loss.item()\n","            epoch_accuracy += accuracy.item()\n","\n","    len_iterator = len(iterator)\n","    return epoch_loss/len_iterator , epoch_accuracy/len_iterator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NzEQR-YCdMv"},"source":["## Training!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nXSPkSOCZAd","executionInfo":{"status":"ok","timestamp":1619955965318,"user_tz":-330,"elapsed":138769,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"43963553-e2db-4229-da98-c9a4d118a54a"},"source":["NUM_EPOCHS = 50\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(NUM_EPOCHS):\n","    train_loss, train_accuracy = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_accuracy = evaluate(model, valid_iterator, optimizer, criterion)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = best_valid_loss\n","        torch.save(model.state_dict(), 'save_weights.pt')\n","\n","    print(\"Epoch: \",epoch)\n","    print(f'Train Loss: {train_loss:.3f} || Train Accuracy: {train_accuracy*100:.2f}%')\n","    print(f'Validation Loss: {valid_loss:3f} || Validation Accuracy: {valid_accuracy*100:.2f}% \\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:  0\n","Train Loss: 1.057 || Train Accuracy: 42.07%\n","Validation Loss: 1.055970 || Validation Accuracy: 42.38% \n","\n","Epoch:  1\n","Train Loss: 1.048 || Train Accuracy: 43.83%\n","Validation Loss: 1.040945 || Validation Accuracy: 46.04% \n","\n","Epoch:  2\n","Train Loss: 1.019 || Train Accuracy: 50.19%\n","Validation Loss: 0.997919 || Validation Accuracy: 54.19% \n","\n","Epoch:  3\n","Train Loss: 0.984 || Train Accuracy: 54.68%\n","Validation Loss: 0.983854 || Validation Accuracy: 55.56% \n","\n","Epoch:  4\n","Train Loss: 0.956 || Train Accuracy: 58.18%\n","Validation Loss: 0.967743 || Validation Accuracy: 58.00% \n","\n","Epoch:  5\n","Train Loss: 0.936 || Train Accuracy: 60.79%\n","Validation Loss: 0.970905 || Validation Accuracy: 57.32% \n","\n","Epoch:  6\n","Train Loss: 0.922 || Train Accuracy: 61.90%\n","Validation Loss: 0.969928 || Validation Accuracy: 56.71% \n","\n","Epoch:  7\n","Train Loss: 0.910 || Train Accuracy: 63.06%\n","Validation Loss: 0.952413 || Validation Accuracy: 58.69% \n","\n","Epoch:  8\n","Train Loss: 0.902 || Train Accuracy: 63.85%\n","Validation Loss: 0.965344 || Validation Accuracy: 56.94% \n","\n","Epoch:  9\n","Train Loss: 0.892 || Train Accuracy: 65.37%\n","Validation Loss: 0.944767 || Validation Accuracy: 59.98% \n","\n","Epoch:  10\n","Train Loss: 0.881 || Train Accuracy: 66.21%\n","Validation Loss: 0.943567 || Validation Accuracy: 59.45% \n","\n","Epoch:  11\n","Train Loss: 0.884 || Train Accuracy: 65.87%\n","Validation Loss: 0.938348 || Validation Accuracy: 60.44% \n","\n","Epoch:  12\n","Train Loss: 0.869 || Train Accuracy: 67.46%\n","Validation Loss: 0.939918 || Validation Accuracy: 60.29% \n","\n","Epoch:  13\n","Train Loss: 0.865 || Train Accuracy: 67.87%\n","Validation Loss: 0.942434 || Validation Accuracy: 59.30% \n","\n","Epoch:  14\n","Train Loss: 0.862 || Train Accuracy: 68.26%\n","Validation Loss: 0.945231 || Validation Accuracy: 59.60% \n","\n","Epoch:  15\n","Train Loss: 0.852 || Train Accuracy: 69.64%\n","Validation Loss: 0.933484 || Validation Accuracy: 61.28% \n","\n","Epoch:  16\n","Train Loss: 0.850 || Train Accuracy: 69.53%\n","Validation Loss: 0.949121 || Validation Accuracy: 59.15% \n","\n","Epoch:  17\n","Train Loss: 0.849 || Train Accuracy: 69.65%\n","Validation Loss: 0.947874 || Validation Accuracy: 58.46% \n","\n","Epoch:  18\n","Train Loss: 0.839 || Train Accuracy: 70.66%\n","Validation Loss: 0.942094 || Validation Accuracy: 58.92% \n","\n","Epoch:  19\n","Train Loss: 0.838 || Train Accuracy: 70.84%\n","Validation Loss: 0.947598 || Validation Accuracy: 58.99% \n","\n","Epoch:  20\n","Train Loss: 0.825 || Train Accuracy: 72.13%\n","Validation Loss: 0.960216 || Validation Accuracy: 57.62% \n","\n","Epoch:  21\n","Train Loss: 0.824 || Train Accuracy: 72.29%\n","Validation Loss: 0.962653 || Validation Accuracy: 57.24% \n","\n","Epoch:  22\n","Train Loss: 0.821 || Train Accuracy: 72.60%\n","Validation Loss: 0.938965 || Validation Accuracy: 59.53% \n","\n","Epoch:  23\n","Train Loss: 0.818 || Train Accuracy: 72.78%\n","Validation Loss: 0.952451 || Validation Accuracy: 58.31% \n","\n","Epoch:  24\n","Train Loss: 0.807 || Train Accuracy: 73.89%\n","Validation Loss: 0.944392 || Validation Accuracy: 58.84% \n","\n","Epoch:  25\n","Train Loss: 0.805 || Train Accuracy: 73.97%\n","Validation Loss: 0.972792 || Validation Accuracy: 55.64% \n","\n","Epoch:  26\n","Train Loss: 0.797 || Train Accuracy: 74.91%\n","Validation Loss: 0.967725 || Validation Accuracy: 55.95% \n","\n","Epoch:  27\n","Train Loss: 0.797 || Train Accuracy: 74.94%\n","Validation Loss: 1.001456 || Validation Accuracy: 53.20% \n","\n","Epoch:  28\n","Train Loss: 0.787 || Train Accuracy: 75.82%\n","Validation Loss: 0.973252 || Validation Accuracy: 54.73% \n","\n","Epoch:  29\n","Train Loss: 0.785 || Train Accuracy: 76.22%\n","Validation Loss: 0.967270 || Validation Accuracy: 55.87% \n","\n","Epoch:  30\n","Train Loss: 0.786 || Train Accuracy: 75.97%\n","Validation Loss: 0.976258 || Validation Accuracy: 54.95% \n","\n","Epoch:  31\n","Train Loss: 0.776 || Train Accuracy: 76.96%\n","Validation Loss: 0.989299 || Validation Accuracy: 54.27% \n","\n","Epoch:  32\n","Train Loss: 0.775 || Train Accuracy: 77.38%\n","Validation Loss: 0.955392 || Validation Accuracy: 57.93% \n","\n","Epoch:  33\n","Train Loss: 0.776 || Train Accuracy: 77.05%\n","Validation Loss: 0.962094 || Validation Accuracy: 57.85% \n","\n","Epoch:  34\n","Train Loss: 0.774 || Train Accuracy: 77.50%\n","Validation Loss: 0.999908 || Validation Accuracy: 53.05% \n","\n","Epoch:  35\n","Train Loss: 0.771 || Train Accuracy: 77.53%\n","Validation Loss: 0.987910 || Validation Accuracy: 53.58% \n","\n","Epoch:  36\n","Train Loss: 0.764 || Train Accuracy: 78.20%\n","Validation Loss: 0.997418 || Validation Accuracy: 52.59% \n","\n","Epoch:  37\n","Train Loss: 0.761 || Train Accuracy: 78.61%\n","Validation Loss: 1.009454 || Validation Accuracy: 50.99% \n","\n","Epoch:  38\n","Train Loss: 0.760 || Train Accuracy: 78.74%\n","Validation Loss: 0.988524 || Validation Accuracy: 53.81% \n","\n","Epoch:  39\n","Train Loss: 0.759 || Train Accuracy: 78.44%\n","Validation Loss: 1.006024 || Validation Accuracy: 52.06% \n","\n","Epoch:  40\n","Train Loss: 0.751 || Train Accuracy: 79.81%\n","Validation Loss: 0.974219 || Validation Accuracy: 55.49% \n","\n","Epoch:  41\n","Train Loss: 0.749 || Train Accuracy: 79.97%\n","Validation Loss: 0.963826 || Validation Accuracy: 56.55% \n","\n","Epoch:  42\n","Train Loss: 0.751 || Train Accuracy: 79.43%\n","Validation Loss: 1.016852 || Validation Accuracy: 51.14% \n","\n","Epoch:  43\n","Train Loss: 0.750 || Train Accuracy: 79.55%\n","Validation Loss: 0.998365 || Validation Accuracy: 52.29% \n","\n","Epoch:  44\n","Train Loss: 0.739 || Train Accuracy: 80.89%\n","Validation Loss: 0.985950 || Validation Accuracy: 55.26% \n","\n","Epoch:  45\n","Train Loss: 0.745 || Train Accuracy: 80.30%\n","Validation Loss: 0.967041 || Validation Accuracy: 55.72% \n","\n","Epoch:  46\n","Train Loss: 0.737 || Train Accuracy: 81.01%\n","Validation Loss: 0.971035 || Validation Accuracy: 56.02% \n","\n","Epoch:  47\n","Train Loss: 0.734 || Train Accuracy: 81.21%\n","Validation Loss: 0.981661 || Validation Accuracy: 55.03% \n","\n","Epoch:  48\n","Train Loss: 0.727 || Train Accuracy: 82.13%\n","Validation Loss: 1.004937 || Validation Accuracy: 52.36% \n","\n","Epoch:  49\n","Train Loss: 0.733 || Train Accuracy: 81.56%\n","Validation Loss: 0.995236 || Validation Accuracy: 54.42% \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-fhqPxs34jtG"},"source":["### Result:\n"," Overfiiting reduced!\n","\n"," Till epoch 12-13 it gave good results but then it started overfitting again. And Validation loss was stuck near 59%."]},{"cell_type":"markdown","metadata":{"id":"zPkLOZOMFONJ"},"source":["# Model Testing"]},{"cell_type":"code","metadata":{"id":"OovBNkLJDe5W"},"source":["path = './save_weights.pt'\n","model.load_state_dict(torch.load(path))\n","model.eval()\n","\n","tokenizer_file = open('./tokenizer.pkl', 'rb')\n","tokenizer = pickle.load(tokenizer_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rlxvdyUlFsx4"},"source":["import spacy\n","\n","nlp = spacy.load('en')\n","\n","def classify_tweet(tweet):\n","    labels = {0:'Negative', 1: 'Positive', 2:'Neutral'}\n","\n","    # tokenized the tweet\n","    tokenized = [t.text for t in nlp.tokenizer(tweet)]\n","\n","    # convert to integer sequence using predefined tokenizer dictionary\n","    indexed = [tokenizer[t] for t in tokenized]\n","\n","    # compute number of words\n","    length = [len(indexed)]\n","\n","    # convert to tensor\n","    tensor = torch.LongTensor(indexed).to(device)\n","\n","    # reshape in form of [batch, number of words]\n","    tensor = tensor.unsqueeze(1).T\n","\n","    # convert to tensor\n","    length_tensor = torch.LongTensor(length)\n","\n","    # get the prediction\n","    prediction  = model(tensor, length_tensor)\n","\n","    _,prediction = torch.max(prediction, 1)\n","\n","    return labels[prediction.item()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"5JrGQb7SH3S-","executionInfo":{"status":"ok","timestamp":1619955778307,"user_tz":-330,"elapsed":896,"user":{"displayName":"Rakhee Sharma","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhrN1HTbe3PmiQ4JAYte1nDlWqtUKQB17E-XIAWsNY=s64","userId":"00377729413310413662"}},"outputId":"03ef4b68-9aed-4fec-8016-6d7b900d1480"},"source":["classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Positive'"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"Bw6_har7H4-G"},"source":[""],"execution_count":null,"outputs":[]}]}